{ "class": "TreeModel",
  "nodeDataArray": [
{"brush":"lightgray","key":0,"loc":"0 0","text":"Concrete Problems in AI Safety"},
{"brush":"lightblue","dir":"right","key":1,"parent":0,"text":"Introduction","loc":"201.35888671875 -769.5"},
{"brush":"lightblue","dir":"right","key":11,"parent":1,"text":"Rapid progress in AI & ML raises concerns about societal impact","loc":"289.29150390625 -795.5"},
{"brush":"lightblue","dir":"right","key":12,"parent":1,"text":"Focus on accidents: unintended and harmful behavior from poor AI system design","loc":"289.29150390625 -769.5"},
{"brush":"lightblue","dir":"right","key":13,"parent":1,"text":"Goal: Highlight concrete safety problems relevant to cutting-edge AI","loc":"289.29150390625 -743.5"},
{"brush":"lightgreen","dir":"right","key":2,"parent":0,"text":"Overview of Research Problems","loc":"201.35888671875 -626.5"},
{"brush":"lightgreen","dir":"right","key":21,"parent":2,"text":"Categorizing Accidents","loc":"407.7578125 -678.5"},
{"brush":"lightgreen","dir":"right","key":211,"parent":21,"text":"Wrong Objective Function","loc":"560.7158203125 -717.5"},
{"brush":"lightgreen","dir":"right","key":2111,"parent":211,"text":"Avoiding Side Effects (Section 3)","loc":"730.04443359375 -730.5"},
{"brush":"lightgreen","dir":"right","key":2112,"parent":211,"text":"Avoiding Reward Hacking (Section 4)","loc":"730.04443359375 -704.5"},
{"brush":"lightgreen","dir":"right","key":212,"parent":21,"text":"Expensive Objective Function","loc":"560.7158203125 -678.5"},
{"brush":"lightgreen","dir":"right","key":2121,"parent":212,"text":"Scalable Supervision (Section 5)","loc":"751.96923828125 -678.5"},
{"brush":"lightgreen","dir":"right","key":213,"parent":21,"text":"Undesirable Behavior During Learning","loc":"560.7158203125 -639.5"},
{"brush":"lightgreen","dir":"right","key":2131,"parent":213,"text":"Safe Exploration (Section 6)","loc":"801.12548828125 -652.5"},
{"brush":"lightgreen","dir":"right","key":2132,"parent":213,"text":"Robustness to Distributional Shift (Section 7)","loc":"801.12548828125 -626.5"},
{"brush":"lightgreen","dir":"right","key":22,"parent":2,"text":"Trends Amplifying Safety Problems","loc":"407.7578125 -574.5"},
{"brush":"lightgreen","dir":"right","key":221,"parent":22,"text":"Increasing use of Reinforcement Learning (RL)","loc":"630.298828125 -600.5"},
{"brush":"lightgreen","dir":"right","key":222,"parent":22,"text":"More complex agents and environments","loc":"630.298828125 -574.5"},
{"brush":"lightgreen","dir":"right","key":223,"parent":22,"text":"Growing autonomy in AI systems","loc":"630.298828125 -548.5"},
{"brush":"lightyellow","dir":"right","key":3,"parent":0,"text":"Avoiding Negative Side Effects","loc":"201.35888671875 -467.25"},
{"brush":"lightyellow","dir":"right","key":31,"parent":3,"text":"Problem: Optimizing one aspect of the environment can negatively impact others","loc":"398.6552734375 -522.5"},
{"brush":"lightyellow","dir":"right","key":32,"parent":3,"text":"Approaches","loc":"398.6552734375 -412"},
{"brush":"lightyellow","dir":"right","key":321,"parent":32,"text":"Define an Impact Regularizer","loc":"488.03515625 -483.5"},
{"brush":"lightyellow","dir":"right","key":3211,"parent":321,"text":"Penalize 'change to the environment'","loc":"676.39404296875 -496.5"},
{"brush":"lightyellow","dir":"right","key":3212,"parent":321,"text":"Challenges in defining and measuring 'change'","loc":"676.39404296875 -470.5"},
{"brush":"lightyellow","dir":"right","key":322,"parent":32,"text":"Learn an Impact Regularizer","loc":"488.03515625 -444.5"},
{"brush":"lightyellow","dir":"right","key":3221,"parent":322,"text":"Use transfer learning to train a generalized impact regularizer","loc":"672.06494140625 -444.5"},
{"brush":"lightyellow","dir":"right","key":323,"parent":32,"text":"Penalize Influence","loc":"488.03515625 -418.5"},
{"brush":"lightyellow","dir":"right","key":3231,"parent":323,"text":"Minimize information-theoretic measures of influence (e.g., empowerment)","loc":"614.27587890625 -418.5"},
{"brush":"lightyellow","dir":"right","key":324,"parent":32,"text":"Multi-Agent Approaches","loc":"488.03515625 -379.5"},
{"brush":"lightyellow","dir":"right","key":3241,"parent":324,"text":"Cooperative Inverse Reinforcement Learning","loc":"646.77587890625 -392.5"},
{"brush":"lightyellow","dir":"right","key":3242,"parent":324,"text":"Reward Autoencoder for 'goal transparency'","loc":"646.77587890625 -366.5"},
{"brush":"lightyellow","dir":"right","key":325,"parent":32,"text":"Reward Uncertainty","loc":"488.03515625 -340.5"},
{"brush":"lightyellow","dir":"right","key":3251,"parent":325,"text":"Model uncertainty about the reward function to discourage large environmental impact","loc":"622.19140625 -340.5"},
{"brush":"lightpink","dir":"right","key":4,"parent":0,"text":"Avoiding Reward Hacking","loc":"201.35888671875 -165"},
{"brush":"lightpink","dir":"right","key":41,"parent":4,"text":"Problem: Agents exploiting flaws in reward function to maximize reward in unintended ways","loc":"369.9765625 -314.5"},
{"brush":"lightpink","dir":"right","key":42,"parent":4,"text":"Causes","loc":"369.9765625 -223.5"},
{"brush":"lightpink","dir":"right","key":421,"parent":42,"text":"Partially Observed Goals","loc":"434.0546875 -288.5"},
{"brush":"lightpink","dir":"right","key":422,"parent":42,"text":"Complicated Systems","loc":"434.0546875 -262.5"},
{"brush":"lightpink","dir":"right","key":423,"parent":42,"text":"Abstract Rewards","loc":"434.0546875 -236.5"},
{"brush":"lightpink","dir":"right","key":424,"parent":42,"text":"Goodhart's Law","loc":"434.0546875 -210.5"},
{"brush":"lightpink","dir":"right","key":425,"parent":42,"text":"Feedback Loops","loc":"434.0546875 -184.5"},
{"brush":"lightpink","dir":"right","key":426,"parent":42,"text":"Environmental Embedding","loc":"434.0546875 -158.5"},
{"brush":"lightpink","dir":"right","key":43,"parent":4,"text":"Approaches","loc":"369.9765625 -15.5"},
{"brush":"lightpink","dir":"right","key":431,"parent":43,"text":"Adversarial Reward Functions","loc":"459.3564453125 -132.5"},
{"brush":"lightpink","dir":"right","key":432,"parent":43,"text":"Model Lookahead","loc":"459.3564453125 -106.5"},
{"brush":"lightpink","dir":"right","key":433,"parent":43,"text":"Adversarial Blinding","loc":"459.3564453125 -80.5"},
{"brush":"lightpink","dir":"right","key":434,"parent":43,"text":"Careful Engineering","loc":"459.3564453125 -54.5"},
{"brush":"lightpink","dir":"right","key":435,"parent":43,"text":"Reward Capping","loc":"459.3564453125 -28.5"},
{"brush":"lightpink","dir":"right","key":436,"parent":43,"text":"Counterexample Resistance","loc":"459.3564453125 -2.5"},
{"brush":"lightpink","dir":"right","key":437,"parent":43,"text":"Multiple Rewards","loc":"459.3564453125 23.5"},
{"brush":"lightpink","dir":"right","key":438,"parent":43,"text":"Reward Pretraining","loc":"459.3564453125 49.5"},
{"brush":"lightpink","dir":"right","key":439,"parent":43,"text":"Variable Indifference","loc":"459.3564453125 75.5"},
{"brush":"lightpink","dir":"right","key":4310,"parent":43,"text":"Trip Wires","loc":"459.3564453125 101.5"},
{"brush":"lightsalmon","dir":"right","key":5,"parent":0,"text":"Scalable Oversight","loc":"201.35888671875 163.25"},
{"brush":"lightsalmon","dir":"right","key":51,"parent":5,"text":"Problem: Limited oversight budget to evaluate complex objectives during training","loc":"330.46875 127.5"},
{"brush":"lightsalmon","dir":"right","key":52,"parent":5,"text":"Approaches","loc":"330.46875 199"},
{"brush":"lightsalmon","dir":"right","key":521,"parent":52,"text":"Semi-supervised Reinforcement Learning","loc":"419.8486328125 166.5"},
{"brush":"lightsalmon","dir":"right","key":5211,"parent":521,"text":"Train on limited labeled data and leverage unlabeled data","loc":"679.02197265625 153.5"},
{"brush":"lightsalmon","dir":"right","key":5212,"parent":521,"text":"Identify and learn proxies for the true reward function","loc":"679.02197265625 179.5"},
{"brush":"lightsalmon","dir":"right","key":522,"parent":52,"text":"Distant Supervision","loc":"419.8486328125 205.5"},
{"brush":"lightsalmon","dir":"right","key":5221,"parent":522,"text":"Provide aggregate or noisy information about decisions instead of individual evaluations","loc":"551.8466796875 205.5"},
{"brush":"lightsalmon","dir":"right","key":523,"parent":52,"text":"Hierarchical Reinforcement Learning","loc":"419.8486328125 231.5"},
{"brush":"lightsalmon","dir":"right","key":5231,"parent":523,"text":"Top-level agent learns from sparse rewards, sub-agents receive dense synthetic rewards","loc":"650.83837890625 231.5"},
{"brush":"lightcoral","dir":"right","key":6,"parent":0,"text":"Safe Exploration","loc":"201.35888671875 303"},
{"brush":"lightcoral","dir":"right","key":61,"parent":6,"text":"Problem: Exploration necessary for learning, but can be dangerous in real-world environments","loc":"316.75146484375 257.5"},
{"brush":"lightcoral","dir":"right","key":62,"parent":6,"text":"Approaches","loc":"316.75146484375 348.5"},
{"brush":"lightcoral","dir":"right","key":621,"parent":62,"text":"Risk-Sensitive Performance Criteria","loc":"406.13134765625 283.5"},
{"brush":"lightcoral","dir":"right","key":6211,"parent":621,"text":"Optimize for worst-case performance, limit probability of bad outcomes, or penalize variance","loc":"632.02392578125 283.5"},
{"brush":"lightcoral","dir":"right","key":622,"parent":62,"text":"Use Demonstrations","loc":"406.13134765625 309.5"},
{"brush":"lightcoral","dir":"right","key":6221,"parent":622,"text":"Utilize inverse RL or apprenticeship learning to learn from expert trajectories","loc":"543.8994140625 309.5"},
{"brush":"lightcoral","dir":"right","key":623,"parent":62,"text":"Simulated Exploration","loc":"406.13134765625 335.5"},
{"brush":"lightcoral","dir":"right","key":6231,"parent":623,"text":"Conduct exploration in simulated environments to minimize real-world risks","loc":"552.58935546875 335.5"},
{"brush":"lightcoral","dir":"right","key":624,"parent":62,"text":"Bounded Exploration","loc":"406.13134765625 361.5"},
{"brush":"lightcoral","dir":"right","key":6241,"parent":624,"text":"Restrict exploration to a safe region of the state space","loc":"546.83203125 361.5"},
{"brush":"lightcoral","dir":"right","key":625,"parent":62,"text":"Trusted Policy Oversight","loc":"406.13134765625 387.5"},
{"brush":"lightcoral","dir":"right","key":6251,"parent":625,"text":"Limit exploration to actions deemed recoverable by a trusted policy","loc":"567.25244140625 387.5"},
{"brush":"lightcoral","dir":"right","key":626,"parent":62,"text":"Human Oversight","loc":"406.13134765625 413.5"},
{"brush":"lightcoral","dir":"right","key":6261,"parent":626,"text":"Consult humans to evaluate potentially unsafe actions","loc":"527.28125 413.5"},
{"brush":"lightcyan","dir":"right","key":7,"parent":0,"text":"Robustness to Distributional Change","loc":"201.35888671875 537"},
{"brush":"lightcyan","dir":"right","key":71,"parent":7,"text":"Problem: ML systems may perform poorly and misjudge performance when test distribution differs from training distribution","loc":"432.36767578125 439.5"},
{"brush":"lightcyan","dir":"right","key":72,"parent":7,"text":"Approaches","loc":"432.36767578125 524"},
{"brush":"lightcyan","dir":"right","key":721,"parent":72,"text":"Well-specified Models","loc":"521.74755859375 478.5"},
{"brush":"lightcyan","dir":"right","key":7211,"parent":721,"text":"Covariate Shift: Re-weight training examples based on the ratio of test and training densities","loc":"667.9453125 465.5"},
{"brush":"lightcyan","dir":"right","key":7212,"parent":721,"text":"Marginal Likelihood: Assume a well-specified model family and account for finite-sample variance","loc":"667.9453125 491.5"},
{"brush":"lightcyan","dir":"right","key":722,"parent":72,"text":"Partially Specified Models","loc":"521.74755859375 530.5"},
{"brush":"lightcyan","dir":"right","key":7221,"parent":722,"text":"Method of Moments: Identify parameters based on limited assumptions about the distribution","loc":"690.59375 517.5"},
{"brush":"lightcyan","dir":"right","key":7222,"parent":722,"text":"Unsupervised Risk Estimation: Model the distribution of errors to assess performance","loc":"690.59375 543.5"},
{"brush":"lightcyan","dir":"right","key":723,"parent":72,"text":"Training on Multiple Distributions","loc":"521.74755859375 569.5"},
{"brush":"lightcyan","dir":"right","key":7231,"parent":723,"text":"Train on diverse datasets to improve generalization to novel distributions","loc":"729.84765625 569.5"},
{"brush":"lightcyan","dir":"right","key":73,"parent":7,"text":"Responding to Out-of-Distribution Inputs","loc":"432.36767578125 595.5"},
{"brush":"lightcyan","dir":"right","key":731,"parent":73,"text":"Seek human input, pinpoint uncertain aspects of predictions, or take conservative actions","loc":"684.32373046875 595.5"},
{"brush":"lightcyan","dir":"right","key":74,"parent":7,"text":"Unifying Views","loc":"432.36767578125 634.5"},
{"brush":"lightcyan","dir":"right","key":741,"parent":74,"text":"Counterfactual Reasoning: Analyze 'what if' scenarios to understand distributional shifts","loc":"537.38818359375 621.5"},
{"brush":"lightcyan","dir":"right","key":742,"parent":74,"text":"Machine Learning with Contracts: Design systems that satisfy well-defined behavioral contracts","loc":"537.38818359375 647.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":8,"parent":0,"text":"Related Efforts","loc":"201.35888671875 712.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":81,"parent":8,"text":"Cyber-Physical Systems Community","loc":"307.109375 673.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":811,"parent":81,"text":"Focus on security and safety of systems interacting with the physical world","loc":"537.32470703125 673.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":82,"parent":8,"text":"Futurist Community","loc":"307.109375 699.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":821,"parent":82,"text":"Concerned with long-term implications of AI, particularly superintelligence","loc":"439.79931640625 699.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":83,"parent":8,"text":"Other Calls for Work on Safety","loc":"307.109375 725.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":831,"parent":83,"text":"Open Letter (2015), research priorities for robust AI, informal writings on AI safety","loc":"503.1552734375 725.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":84,"parent":8,"text":"Related Problems in Safety","loc":"307.109375 751.5"},
{"brush":"lightgoldenrodyellow","dir":"right","key":841,"parent":84,"text":"Privacy, Fairness, Security, Abuse, Transparency, Policy","loc":"484.6328125 751.5"},
{"brush":"lavender","dir":"right","key":9,"parent":0,"text":"Conclusion","loc":"201.35888671875 790.5"},
{"brush":"lavender","dir":"right","key":91,"parent":9,"text":"Small-scale accidents are a concrete threat, larger accidents a concern","loc":"285.67333984375 777.5"},
{"brush":"lavender","dir":"right","key":92,"parent":9,"text":"Need for a unified approach to prevent unintended harm from autonomous systems","loc":"285.67333984375 803.5"}
]}